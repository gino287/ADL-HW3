# ADL HW3 - Retrieval-Augmented Generation (RAG)

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.8.0-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/Transformers-4.56.1-yellow.svg)](https://huggingface.co/transformers/)

æœ¬å°ˆæ¡ˆå¯¦ä½œä¸€å€‹å®Œæ•´çš„ RAG ç³»çµ±ï¼ŒåŒ…å« **Retriever**ï¼ˆé›™å¡”æ¨¡å‹ï¼‰ã€**Reranker**ï¼ˆCross-Encoderï¼‰å’Œ **Generator**ï¼ˆLLMï¼‰ä¸‰å€‹éšæ®µï¼Œç”¨æ–¼å•ç­”ä»»å‹™ã€‚

---

## ğŸ“‹ ç›®éŒ„

- [å°ˆæ¡ˆæ¶æ§‹](#å°ˆæ¡ˆæ¶æ§‹)
- [ç’°å¢ƒè¨­ç½®](#ç’°å¢ƒè¨­ç½®)
- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [å®Œæ•´è¨“ç·´æµç¨‹](#å®Œæ•´è¨“ç·´æµç¨‹)
- [æ¨è«–èˆ‡è©•ä¼°](#æ¨è«–èˆ‡è©•ä¼°)
- [å·¥å…·èªªæ˜](#å·¥å…·èªªæ˜)
- [å¯¦é©—é‡ç¾](#å¯¦é©—é‡ç¾)
- [ç›®éŒ„çµæ§‹](#ç›®éŒ„çµæ§‹)

---

## ğŸ—ï¸ å°ˆæ¡ˆæ¶æ§‹

```
RAG Pipeline: Query â†’ Retriever â†’ Reranker â†’ Generator â†’ Answer

1. Retriever (Bi-Encoder): å¾ corpus ä¸­æª¢ç´¢ Top-K ç›¸é—œæ®µè½
2. Reranker (Cross-Encoder): é‡æ–°æ’åºï¼Œé¸å‡º Top-M æœ€ç›¸é—œæ®µè½
3. Generator (LLM): åŸºæ–¼æª¢ç´¢åˆ°çš„æ®µè½ç”Ÿæˆç­”æ¡ˆ
```

---

## ğŸ› ï¸ ç’°å¢ƒè¨­ç½®

### ç³»çµ±éœ€æ±‚
- **Python**: 3.12
- **CUDA**: 12.4 (for GPU support)
- **GPU**: å»ºè­°ä½¿ç”¨ A100 (è‡³å°‘ 16GB VRAM)

### å®‰è£æ­¥é©Ÿ

#### 1. Clone å°ˆæ¡ˆ
```bash
git clone https://github.com/gino287/ADL-HW3.git
cd ADL-HW3
```

#### 2. å»ºç«‹è™›æ“¬ç’°å¢ƒï¼ˆå»ºè­°ï¼‰
```bash
python3.12 -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate  # Windows
```

#### 3. å®‰è£å¥—ä»¶
```bash
pip install -r requirements.txt
```

**requirements.txt å…§å®¹ï¼š**
- `transformers==4.56.1` - HuggingFace æ¨¡å‹åº«
- `torch==2.8.0` - PyTorch (CUDA 12.4)
- `datasets==4.0.0` - è³‡æ–™é›†è™•ç†
- `sentence-transformers==5.1.0` - é›™å¡”æ¨¡å‹èˆ‡ Cross-Encoder
- `faiss-gpu-cu12==1.12.0` - å‘é‡æª¢ç´¢å¼•æ“
- `python-dotenv==1.1.1` - ç’°å¢ƒè®Šæ•¸ç®¡ç†
- `accelerate==1.10.1` - åˆ†æ•£å¼è¨“ç·´åŠ é€Ÿ
- `gdown` - Google Drive ä¸‹è¼‰å·¥å…·

#### 4. è¨­å®š HuggingFace Token
å»ºç«‹ `.env` æª”æ¡ˆä¸¦åŠ å…¥æ‚¨çš„ HuggingFace tokenï¼š
```bash
echo 'hf_token="your_huggingface_token_here"' > .env
```
> å–å¾— token: https://huggingface.co/settings/tokens

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ä¸‹è¼‰é è¨“ç·´æ¨¡å‹
```bash
bash download.sh
```
é€™æœƒä¸‹è¼‰å·²è¨“ç·´å¥½çš„ Retriever å’Œ Reranker æ¨¡å‹åˆ° `models/` ç›®éŒ„ã€‚

### 2. å»ºç«‹å‘é‡è³‡æ–™åº«
```bash
python code/data_preparation/save_embeddings.py \
  --retriever_model_path ./models/retriever \
  --build_db
```

### 3. åŸ·è¡Œæ¨è«–
```bash
python code/evaluation/inference_batch.py \
  --retriever_model_path ./models/retriever \
  --reranker_model_path ./models/reranker \
  --test_data_path ./data/test_open.txt
```

è¼¸å‡ºæª”æ¡ˆï¼š`result.json`

---

## ğŸ“ å®Œæ•´è¨“ç·´æµç¨‹

### Step 1: è³‡æ–™æº–å‚™èˆ‡æª¢æŸ¥

#### 1.1 æª¢æŸ¥è³‡æ–™çµæ§‹
```bash
# æŸ¥çœ‹ JSONL æª”æ¡ˆçµæ§‹
python code/check_tool/hw3_inspect_head.py \
  --train data/train.txt \
  --corpus data/corpus.txt \
  --n 20

# é è¦½å•ç­”å°å…§å®¹
python code/check_tool/hw3_preview_qa.py \
  --train data/train.txt \
  --n 20
```

#### 1.2 çµ±è¨ˆè³‡æ–™ç‰¹æ€§
```bash
# æƒæè¨“ç·´è³‡æ–™çµ±è¨ˆï¼ˆtoken é•·åº¦ã€æ¨™ç±¤åˆ†å¸ƒç­‰ï¼‰
python code/evaluation/scan_train_stats.py \
  --train_path data/train.txt \
  --ce_model cross-encoder/ms-marco-MiniLM-L-12-v2 \
  --limit 200 \
  --csv_out work/train_stats.csv
```

#### 1.3 æª¢æŸ¥è¨“ç·´è³‡æ–™èˆ‡èªæ–™åº«é‡ç–Š
```bash
python code/check_tool/check_train_corpus_overlap.py \
  --train data/train.txt \
  --corpus data/corpus.txt \
  --n 200
```

---

### Step 2: æŒ–æ˜ Hard Negatives

Hard negatives æ˜¯è¨“ç·´ Retriever å’Œ Reranker çš„é—œéµï¼Œèƒ½æå‡æ¨¡å‹è¾¨åˆ¥ç›¸ä¼¼ä½†ä¸ç›¸é—œæ–‡æœ¬çš„èƒ½åŠ›ã€‚

```bash
# å…ˆç”¨åŸºç¤æ¨¡å‹å»ºç«‹å‘é‡è³‡æ–™åº«
python code/data_preparation/save_embeddings.py \
  --retriever_model_path intfloat/multilingual-e5-small \
  --build_db

# æŒ–æ˜ hard negatives
python code/data_preparation/mine_hard_negatives.py \
  --train_path data/train.txt \
  --index_path vector_database/passage_index.faiss \
  --sqlite_path vector_database/passage_store.db \
  --retriever_model intfloat/multilingual-e5-small \
  --topk 50 \
  --per_q_hard 2 \
  --out_path data/hardneg.jsonl
```

#### æª¢æŸ¥ Hard Negatives å“è³ª
```bash
python code/check_tool/check_hardneg.py \
  --train_path data/train.txt \
  --hardneg_path data/hardneg.jsonl \
  --k 12 \
  --topk_check \
  --index_path vector_database/passage_index.faiss \
  --sqlite_path vector_database/passage_store.db \
  --retriever_model intfloat/multilingual-e5-small \
  --topk 50
```

---

### Step 3: è¨“ç·´ Retrieverï¼ˆé›™å¡”æ¨¡å‹ï¼‰

Retriever ä½¿ç”¨ Sentence Transformer çš„é›™å¡”æ¶æ§‹ï¼Œå°‡ query å’Œ passage ç·¨ç¢¼æˆå‘é‡ã€‚

```bash
python code/training/train_retriever.py \
  --train_path data/train.txt \
  --hardneg_path data/hardneg.jsonl \
  --corpus_path data/corpus.txt \
  --output_dir models/retriever \
  --base_model intfloat/multilingual-e5-small \
  --epochs 8 \
  --per_device_train_batch_size 192 \
  --gradient_accumulation_steps 2 \
  --max_seq_length 512 \
  --lr 2e-5 \
  --warmup_ratio 0.08 \
  --save_steps 50 \
  --eval_steps 50 \
  --logging_steps 10 \
  --seed 42
```

**é—œéµåƒæ•¸èªªæ˜ï¼š**
- `--epochs 8`: è¨“ç·´ 8 å€‹ epoch
- `--per_device_train_batch_size 192`: æ¯å¼µ GPU çš„ batch sizeï¼ˆA100 å»ºè­° 192ï¼‰
- `--gradient_accumulation_steps 2`: æ¢¯åº¦ç´¯ç©æ­¥æ•¸ï¼ˆæœ‰æ•ˆ batch size = 192 Ã— 2 = 384ï¼‰
- `--max_seq_length 512`: æœ€å¤§åºåˆ—é•·åº¦
- `--lr 2e-5`: å­¸ç¿’ç‡
- `--warmup_ratio 0.08`: warmup æ¯”ä¾‹ï¼ˆå‰ 8% çš„æ­¥æ•¸é€²è¡Œ warmupï¼‰

**è¨“ç·´å¾Œè¦–è¦ºåŒ–ï¼š**
```bash
python code/training/plot_loss_curves_retriever.py \
  --model_dir models/retriever \
  --out_dir report_artifacts \
  --x_axis steps \
  --save_csv true
```

---

### Step 4: è¨“ç·´ Rerankerï¼ˆCross-Encoderï¼‰

Reranker ä½¿ç”¨ Cross-Encoder æ¶æ§‹ï¼ŒåŒæ™‚ç·¨ç¢¼ query å’Œ passage çš„äº¤äº’ã€‚

```bash
python code/training/train_rerank.py \
  --data_dir data \
  --output_dir models/reranker \
  --profile a100 \
  --epochs 3 \
  --lr 2e-5 \
  --batch_size 128 \
  --max_length 512 \
  --hard_neg_cap 4 \
  --eval_steps 1000 \
  --save_steps 1000 \
  --logging_steps 100 \
  --num_workers 12 \
  --seed 12
```

**é—œéµåƒæ•¸èªªæ˜ï¼š**
- `--profile a100`: ç¡¬é«”é…ç½®ï¼ˆå¯é¸ `a100`, `t4`, `auto`ï¼‰
- `--batch_size 128`: è¨“ç·´ batch sizeï¼ˆA100 å»ºè­° 128ï¼‰
- `--max_length 512`: Cross-Encoder çš„æœ€å¤§è¼¸å…¥é•·åº¦
- `--hard_neg_cap 4`: æ¯å€‹ query æœ€å¤šä½¿ç”¨ 4 å€‹ hard negatives
- `--num_workers 12`: è³‡æ–™è¼‰å…¥çš„ worker æ•¸é‡

**è¨“ç·´å¾Œè¦–è¦ºåŒ–ï¼š**
```bash
python code/training/plot_loss_curves_rerank.py \
  --model_dir models/reranker \
  --out_dir report_artifacts \
  --x_axis steps \
  --save_csv true
```

---

### Step 5: é‡å»ºå‘é‡è³‡æ–™åº«

ä½¿ç”¨è¨“ç·´å¥½çš„ Retriever é‡æ–°å»ºç«‹å‘é‡è³‡æ–™åº«ï¼š

```bash
python code/data_preparation/save_embeddings.py \
  --data_folder ./data \
  --file_name corpus.txt \
  --output_folder ./vector_database \
  --retriever_model_path ./models/retriever \
  --output_index_file_name passage_index.faiss \
  --output_db_file_name passage_store.db \
  --batch_size 256 \
  --build_db
```

**åƒæ•¸èªªæ˜ï¼š**
- `--build_db`: åŒæ™‚å»ºç«‹ SQLite è³‡æ–™åº«ï¼ˆå„²å­˜åŸå§‹æ–‡æœ¬ï¼‰
- `--batch_size 256`: ç·¨ç¢¼ batch size

---

## ğŸ“Š æ¨è«–èˆ‡è©•ä¼°

### å®Œæ•´æ¨è«–ï¼ˆRetriever + Reranker + Generatorï¼‰

```bash
python code/evaluation/inference_batch.py \
  --data_folder ./data \
  --passage_file corpus.txt \
  --index_folder ./vector_database \
  --index_file passage_index.faiss \
  --sqlite_file passage_store.db \
  --test_data_path ./data/test_open.txt \
  --qrels_path ./data/qrels.txt \
  --retriever_model_path ./models/retriever \
  --reranker_model_path ./models/reranker \
  --generator_model Qwen/Qwen3-1.7B \
  --result_file_name result.json
```

**è¼¸å‡ºæ ¼å¼ (`result.json`)ï¼š**
```json
{
  "records": [
    {
      "qid": "...",
      "query": "...",
      "generated": "...",
      "gold_answer": "...",
      "retrieved_passages": [...],
      "reranked_passages": [...]
    }
  ],
  "retrieval_metrics": {...},
  "generation_metrics": {...}
}
```

### åƒ…ä½¿ç”¨ Retrieverï¼ˆç„¡ Rerankerï¼‰

```bash
python code/evaluation/inference_batch_norerank.py \
  --retriever_model_path ./models/retriever \
  --test_data_path ./data/test_open.txt \
  --result_file_name result_norerank.json
```

---

## ğŸ§° å·¥å…·èªªæ˜

### è³‡æ–™æª¢æŸ¥å·¥å…· (`code/check_tool/`)

#### 1. `hw3_inspect_head.py` - JSONL çµæ§‹æª¢æŸ¥å™¨
å¿«é€ŸæŸ¥çœ‹ JSONL æª”æ¡ˆçš„ key çµæ§‹å’Œå€¼é è¦½ã€‚
```bash
python code/check_tool/hw3_inspect_head.py \
  --train data/train.txt \
  --corpus data/corpus.txt \
  --n 20
```

#### 2. `hw3_preview_qa.py` - å•ç­”å°é è¦½
å°ˆé–€æŸ¥çœ‹ questionã€rewriteã€answer å…§å®¹ã€‚
```bash
python code/check_tool/hw3_preview_qa.py \
  --train data/train.txt \
  --n 20
```

#### 3. `check_hardneg.py` - Hard Negatives å“è³ªæª¢æŸ¥
æª¢æŸ¥æŒ–æ˜çš„ hard negatives æ˜¯å¦èˆ‡ gold answer é‡è¤‡ã€æ˜¯å¦åœ¨ Top-K å…§ç­‰ã€‚
```bash
python code/check_tool/check_hardneg.py \
  --train_path data/train.txt \
  --hardneg_path data/hardneg.jsonl \
  --k 12 \
  --topk_check
```

#### 4. `check_train_corpus_overlap.py` - è³‡æ–™é‡ç–Šæª¢æŸ¥
æª¢æŸ¥è¨“ç·´è³‡æ–™çš„ evidence èˆ‡ corpus çš„é‡ç–Šåº¦ã€‚
```bash
python code/check_tool/check_train_corpus_overlap.py \
  --train data/train.txt \
  --corpus data/corpus.txt \
  --n 200
```

---

### è³‡æ–™æº–å‚™å·¥å…· (`code/data_preparation/`)

#### 1. `save_embeddings.py` - å»ºç«‹å‘é‡è³‡æ–™åº«
å°‡ corpus ç·¨ç¢¼æˆå‘é‡ä¸¦å»ºç«‹ FAISS ç´¢å¼•ã€‚
```bash
python code/data_preparation/save_embeddings.py \
  --retriever_model_path intfloat/multilingual-e5-small \
  --build_db
```

#### 2. `mine_hard_negatives.py` - æŒ–æ˜ Hard Negatives
å¾ Top-K æª¢ç´¢çµæœä¸­æŒ–æ˜ hard negativesã€‚
```bash
python code/data_preparation/mine_hard_negatives.py \
  --train_path data/train.txt \
  --topk 50 \
  --per_q_hard 2 \
  --out_path data/hardneg.jsonl
```

---

### è©•ä¼°å·¥å…· (`code/evaluation/`)

#### 1. `scan_train_stats.py` - è¨“ç·´è³‡æ–™çµ±è¨ˆ
æ·±åº¦åˆ†æè¨“ç·´è³‡æ–™çš„çµ±è¨ˆç‰¹æ€§ï¼ˆtoken é•·åº¦ã€æ¨™ç±¤åˆ†å¸ƒç­‰ï¼‰ã€‚
```bash
python code/evaluation/scan_train_stats.py \
  --train_path data/train.txt \
  --limit 200 \
  --csv_out work/train_stats.csv
```

#### 2. `inference_batch.py` - å®Œæ•´æ¨è«–
å®Œæ•´çš„ RAG pipelineï¼ˆRetriever + Reranker + Generatorï¼‰ã€‚

#### 3. `inference_batch_norerank.py` - ç°¡åŒ–æ¨è«–
åƒ…ä½¿ç”¨ Retriever + Generatorï¼ˆç„¡ Rerankerï¼‰ã€‚

---

### è¨“ç·´å·¥å…· (`code/training/`)

#### 1. `train_retriever.py` - Retriever è¨“ç·´
è¨“ç·´é›™å¡”æ¨¡å‹ï¼ˆBi-Encoderï¼‰ã€‚

#### 2. `train_rerank.py` - Reranker è¨“ç·´
è¨“ç·´ Cross-Encoder é‡æ’æ¨¡å‹ã€‚

#### 3. `plot_loss_curves_retriever.py` - Retriever æ›²ç·šè¦–è¦ºåŒ–
ç¹ªè£½ Retriever çš„è¨“ç·´/é©—è­‰ loss æ›²ç·šã€‚
```bash
python code/training/plot_loss_curves_retriever.py \
  --model_dir models/retriever \
  --out_dir report_artifacts
```

#### 4. `plot_loss_curves_rerank.py` - Reranker æ›²ç·šè¦–è¦ºåŒ–
ç¹ªè£½ Reranker çš„è¨“ç·´/é©—è­‰ loss æ›²ç·šã€‚
```bash
python code/training/plot_loss_curves_rerank.py \
  --model_dir models/reranker \
  --out_dir report_artifacts
```

---

## ğŸ”¬ å¯¦é©—é‡ç¾

### å®Œæ•´å¯¦é©—æµç¨‹ï¼ˆå¾é ­é–‹å§‹ï¼‰

```bash
# ========== 1. ç’°å¢ƒæº–å‚™ ==========
pip install -r requirements.txt
echo 'hf_token="your_token"' > .env

# ========== 2. è³‡æ–™æª¢æŸ¥ ==========
# æª¢æŸ¥è³‡æ–™çµæ§‹
python code/check_tool/hw3_inspect_head.py --train data/train.txt --n 20
python code/check_tool/hw3_preview_qa.py --train data/train.txt --n 20

# çµ±è¨ˆåˆ†æ
python code/evaluation/scan_train_stats.py \
  --train_path data/train.txt \
  --limit 200 \
  --csv_out work/train_stats.csv

# ========== 3. æŒ–æ˜ Hard Negatives ==========
# å…ˆç”¨åŸºç¤æ¨¡å‹å»ºç«‹å‘é‡åº«
python code/data_preparation/save_embeddings.py \
  --retriever_model_path intfloat/multilingual-e5-small \
  --build_db

# æŒ–æ˜ hard negatives
python code/data_preparation/mine_hard_negatives.py \
  --train_path data/train.txt \
  --index_path vector_database/passage_index.faiss \
  --sqlite_path vector_database/passage_store.db \
  --retriever_model intfloat/multilingual-e5-small \
  --topk 50 \
  --per_q_hard 2 \
  --out_path data/hardneg.jsonl

# æª¢æŸ¥ hard negatives å“è³ª
python code/check_tool/check_hardneg.py \
  --train_path data/train.txt \
  --hardneg_path data/hardneg.jsonl \
  --k 12 \
  --topk_check

# ========== 4. è¨“ç·´ Retriever ==========
python code/training/train_retriever.py \
  --train_path data/train.txt \
  --hardneg_path data/hardneg.jsonl \
  --corpus_path data/corpus.txt \
  --output_dir models/retriever \
  --base_model intfloat/multilingual-e5-small \
  --epochs 8 \
  --per_device_train_batch_size 192 \
  --gradient_accumulation_steps 2 \
  --max_seq_length 512 \
  --lr 2e-5 \
  --warmup_ratio 0.08 \
  --save_steps 50 \
  --eval_steps 50 \
  --logging_steps 10 \
  --seed 42

# è¦–è¦ºåŒ–è¨“ç·´æ›²ç·š
python code/training/plot_loss_curves_retriever.py \
  --model_dir models/retriever \
  --out_dir report_artifacts

# ========== 5. è¨“ç·´ Reranker ==========
python code/training/train_rerank.py \
  --data_dir data \
  --output_dir models/reranker \
  --profile a100 \
  --epochs 3 \
  --lr 2e-5 \
  --batch_size 128 \
  --max_length 512 \
  --hard_neg_cap 4 \
  --eval_steps 1000 \
  --save_steps 1000 \
  --logging_steps 100 \
  --num_workers 12 \
  --seed 12

# è¦–è¦ºåŒ–è¨“ç·´æ›²ç·š
python code/training/plot_loss_curves_rerank.py \
  --model_dir models/reranker \
  --out_dir report_artifacts

# ========== 6. é‡å»ºå‘é‡è³‡æ–™åº« ==========
python code/data_preparation/save_embeddings.py \
  --retriever_model_path ./models/retriever \
  --build_db

# ========== 7. æ¨è«–èˆ‡è©•ä¼° ==========
# å®Œæ•´æ¨è«–ï¼ˆå« Rerankerï¼‰
python code/evaluation/inference_batch.py \
  --retriever_model_path ./models/retriever \
  --reranker_model_path ./models/reranker \
  --test_data_path ./data/test_open.txt \
  --result_file_name result.json

# ç°¡åŒ–æ¨è«–ï¼ˆç„¡ Rerankerï¼‰- ç”¨æ–¼æ¶ˆèå¯¦é©—
python code/evaluation/inference_batch_norerank.py \
  --retriever_model_path ./models/retriever \
  --test_data_path ./data/test_open.txt \
  --result_file_name result_norerank.json
```

---

## ğŸ“ ç›®éŒ„çµæ§‹

```
ADL-HW3/
â”œâ”€â”€ code/                          # æ ¸å¿ƒç¨‹å¼ç¢¼
â”‚   â”œâ”€â”€ check_tool/                # è³‡æ–™æª¢æŸ¥å·¥å…·
â”‚   â”‚   â”œâ”€â”€ check_hardneg.py
â”‚   â”‚   â”œâ”€â”€ check_train_corpus_overlap.py
â”‚   â”‚   â”œâ”€â”€ hw3_inspect_head.py
â”‚   â”‚   â””â”€â”€ hw3_preview_qa.py
â”‚   â”œâ”€â”€ data_preparation/          # è³‡æ–™æº–å‚™
â”‚   â”‚   â”œâ”€â”€ mine_hard_negatives.py
â”‚   â”‚   â””â”€â”€ save_embeddings.py
â”‚   â”œâ”€â”€ evaluation/                # æ¨è«–èˆ‡è©•ä¼°
â”‚   â”‚   â”œâ”€â”€ inference_batch.py
â”‚   â”‚   â”œâ”€â”€ inference_batch_norerank.py
â”‚   â”‚   â”œâ”€â”€ scan_train_stats.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â””â”€â”€ training/                  # è¨“ç·´è…³æœ¬
â”‚       â”œâ”€â”€ plot_loss_curves_rerank.py
â”‚       â”œâ”€â”€ plot_loss_curves_retriever.py
â”‚       â”œâ”€â”€ train_rerank.py
â”‚       â””â”€â”€ train_retriever.py
â”œâ”€â”€ data/                          # è³‡æ–™é›†
â”‚   â”œâ”€â”€ corpus.txt                 # æ®µè½èªæ–™åº«
â”‚   â”œâ”€â”€ qrels.txt                  # ç›¸é—œæ€§æ¨™è¨»
â”‚   â”œâ”€â”€ test_open.txt              # æ¸¬è©¦è³‡æ–™
â”‚   â”œâ”€â”€ train.txt                  # è¨“ç·´è³‡æ–™
â”‚   â””â”€â”€ hardneg.jsonl              # (ç”Ÿæˆ) Hard negatives
â”œâ”€â”€ vector_database/               # å‘é‡è³‡æ–™åº«
â”‚   â”œâ”€â”€ passage_index.faiss        # FAISS ç´¢å¼•
â”‚   â””â”€â”€ passage_store.db           # SQLite æ–‡æœ¬å„²å­˜
â”œâ”€â”€ models/                        # è¨“ç·´æ¨¡å‹
â”‚   â”œâ”€â”€ retriever/                 # Retriever æ¨¡å‹
â”‚   â””â”€â”€ reranker/                  # Reranker æ¨¡å‹
â”œâ”€â”€ report_artifacts/              # (ç”Ÿæˆ) å ±å‘Šç”¢å‡º
â”‚   â”œâ”€â”€ retriever_loss_curve.png
â”‚   â””â”€â”€ reranker_loss_curve.png
â”œâ”€â”€ download.sh                    # æ¨¡å‹ä¸‹è¼‰è…³æœ¬
â”œâ”€â”€ requirements.txt               # Python å¥—ä»¶æ¸…å–®
â”œâ”€â”€ .env                           # (éœ€å»ºç«‹) HuggingFace token
â””â”€â”€ README.md                      # æœ¬æ–‡ä»¶
```

---

## ğŸ“ æ³¨æ„äº‹é …

### ç¡¬é«”éœ€æ±‚
- **Retriever è¨“ç·´**: å»ºè­°ä½¿ç”¨ A100 (40GB/80GB) æˆ– 2Ã—RTX 4090
- **Reranker è¨“ç·´**: å»ºè­°ä½¿ç”¨ A100 æˆ–å–®å¼µ RTX 3090/4090
- **æ¨è«–**: è‡³å°‘éœ€è¦ 16GB VRAM

### Batch Size èª¿æ•´
æ ¹æ“šæ‚¨çš„ GPU è¨˜æ†¶é«”èª¿æ•´ batch sizeï¼š

| GPU | Retriever Batch Size | Reranker Batch Size |
|-----|---------------------|---------------------|
| A100 (80GB) | 192-256 | 128-192 |
| A100 (40GB) | 128-192 | 96-128 |
| RTX 4090 (24GB) | 64-96 | 48-64 |
| RTX 3090 (24GB) | 64-96 | 48-64 |
| T4 (16GB) | 32-48 | 24-32 |

### è¨“ç·´æ™‚é–“ä¼°è¨ˆ
- **Retriever**: ç´„ 4-6 å°æ™‚ (A100, 8 epochs)
- **Reranker**: ç´„ 2-3 å°æ™‚ (A100, 3 epochs)
- **å‘é‡è³‡æ–™åº«å»ºç«‹**: ç´„ 10-15 åˆ†é˜
- **æ¨è«–** (300 queries): ç´„ 30-45 åˆ†é˜

---

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: CUDA out of memory
**è§£æ±ºæ–¹æ³•ï¼š** é™ä½ batch size æˆ–å¢åŠ  gradient accumulation steps
```bash
# é™ä½ batch size
--per_device_train_batch_size 64

# æˆ–å¢åŠ  gradient accumulation
--gradient_accumulation_steps 4
```

### Q2: HuggingFace token éŒ¯èª¤
**è§£æ±ºæ–¹æ³•ï¼š** ç¢ºèª `.env` æª”æ¡ˆæ ¼å¼æ­£ç¢º
```bash
hf_token="hf_xxxxxxxxxxxxxxxxxxxxx"
```

### Q3: FAISS ç´¢å¼•è¼‰å…¥å¤±æ•—
**è§£æ±ºæ–¹æ³•ï¼š** é‡æ–°å»ºç«‹å‘é‡è³‡æ–™åº«
```bash
python code/data_preparation/save_embeddings.py \
  --retriever_model_path ./models/retriever \
  --build_db
```

### Q4: æ¨è«–é€Ÿåº¦å¤ªæ…¢
**è§£æ±ºæ–¹æ³•ï¼š** èª¿æ•´ batch size åƒæ•¸ï¼ˆåœ¨è…³æœ¬å…§ä¿®æ”¹ `BATCH_Q` å’Œ `BATCH_GEN`ï¼‰

---

## ğŸ“š åƒè€ƒè³‡æ–™

- [Sentence Transformers Documentation](https://www.sbert.net/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)
- [E5 Model Paper](https://arxiv.org/abs/2212.03533)
- [Cross-Encoder for Re-ranking](https://www.sbert.net/examples/applications/cross-encoder/README.html)

---

## ğŸ“§ è¯çµ¡è³‡è¨Š

å¦‚æœ‰å•é¡Œï¼Œè«‹è¯çµ¡ï¼š35049957a@gmail.com

---

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆåƒ…ä¾›å­¸è¡“ç”¨é€”ï¼Œè«‹å‹¿ç”¨æ–¼å•†æ¥­ç”¨é€”ã€‚

---

**æœ€å¾Œæ›´æ–°ï¼š** 2025-11-06
